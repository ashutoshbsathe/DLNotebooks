{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flipkart GRID challenge\n",
    "\n",
    "### Round 2 : Object localization challenge\n",
    "**Team Name** : undergradients <br>\n",
    "**Round 2 Score** : 79.93% accuracy <br>\n",
    "**Participants** : <br>\n",
    "Ashutosh Sathe, Yash Jakhotiya, Prasad Rathod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data access and preprocessing :\n",
    "The data was provided in the form of zipped images file along with a `training.csv` containing ~14000 lines, each describing a bounding box for each of the image.<br>\n",
    "We decided to do preprocessing and EDA on Google Colab instead of local machine due to unavailability of steady internet connection at the time of writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mount Google Drive in Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Python function for downloading files from Google Drive\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file_from_google_drive('1Q-ZY19lGrlvvzkYQvgNIP7H-__g3-W61','images.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm the download by listing all the files\n",
    "!ls -al\n",
    "## Unzip the images.zip. Since archive contains ~54000 images, \n",
    "## output of this command can be too large, thus we just print a \n",
    "## dot after extracting every 100 files\n",
    "!unzip images.zip | awk 'BEGIN {ORS=\" \"}{if(NR%50==0)print \".\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of images extracted\n",
    "!ls\n",
    "!ls images -l | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## `training.csv` has been uploaded to Google Drive with the name `training`\n",
    "import pandas as pd\n",
    "csv_train = pd.read_csv('/content/drive/My Drive/training')\n",
    "csv_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we donot have enough memory to load all the ~14000 images in our memory, we divide it into small chunks of 1000 images. We read batches of 1000 images in NumPy arrays and store every array on disk using `np.save()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "X_train_data = []\n",
    "i = 0\n",
    "files = csv_train['image_name'].values\n",
    "for file in files:\n",
    "    image = cv2.imread('images/' + file)\n",
    "    i = i + 1\n",
    "    X_train_data.append(image)\n",
    "    if(i % 500 == 0):\n",
    "        print(\"Read {} images\".format(i))\n",
    "    if(i % 1000 == 0):\n",
    "        X_train_data = np.array(X_train_data)\n",
    "        print(X_train_data.shape)\n",
    "        print(\"Saving...\")\n",
    "        np.save(\"X_train_batch_{}.npy\".format(i/1000), X_train_data)\n",
    "        del X_train_data\n",
    "        X_train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy all the batches on Google Drive\n",
    "## Now these `.npy` files can be easily downloaded from Google Drive\n",
    "## The size will be much smaller than original since Google Drive compresses them as well\n",
    "!ls -al\n",
    "!cp X_train_batch_*.npy '/content/drive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing \n",
    "Images of size 640x480 are too heavy for our laptops to handle. Thus we resize the images to be of size 192x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from skimage.transform import resize\n",
    "\n",
    "ORIG_PREFIX = './data/X_train_batch_'\n",
    "ORIG_SUFFIX = '.0.npy'\n",
    "\n",
    "RESIZE_PREFIX = './data/X_resized_train_batch_'\n",
    "RESIZE_SUFFIX = '.npz'\n",
    "for i in range(1, 15):\n",
    "    X_resized_train = []\n",
    "    X_train = np.load(ORIG_PREFIX + str(i) + ORIG_SUFFIX)\n",
    "    for img in X_train:\n",
    "        img_resized = resize(img, (192, 256))\n",
    "        X_resized_train.append(img_resized)\n",
    "    X_resized_train = np.asarray(X_resized_train)\n",
    "    print(\"Resized shape for batch {} is {}\".format(i, X_resized_train.shape))\n",
    "    np.savez_compressed(RESIZE_PREFIX + str(i) + RESIZE_SUFFIX, X_resized_train)\n",
    "    del X_train\n",
    "    del X_resized_train\n",
    "    print(\"Iteration complete\")\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline and Model\n",
    "The next section describes our training pipeline and model.The training pipeline needed to be resource efficient since we had only 2 GPUs with us. The GPUs we used were laptop versions of NVIDIA GeForce 940MX(4GB) and GeForce 1050Ti(4GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the training csv file\n",
    "## We optimize our bounding box prediction model for (x1, y1, width, height)\n",
    "## where (x1, y1) is bottom left corner of the bounding box.\n",
    "\n",
    "Y_train = pd.read_csv('./training')\n",
    "Y_train = Y_train.drop('image_name', axis=1)\n",
    "Y_train['width'] = Y_train['x2'] - Y_train['x1']\n",
    "Y_train['height'] = Y_train['y2'] - Y_train['y1']\n",
    "\n",
    "Y_train = Y_train.drop('x2', axis=1)\n",
    "Y_train = Y_train.drop('y2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(self):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, \n",
    "                     input_shape=(192, 256, 3), data_format='channels_last'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2D(64, kernel_size=5,\n",
    "                      data_format='channels_last'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1),\n",
    "    data_format='channels_last'))\n",
    "    model.add(Conv2D(64, kernel_size=7, strides=(2, 2),\n",
    "                      data_format='channels_last'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "    model.add(Conv2D(128, kernel_size=9, strides=(2, 2),\n",
    "                      data_format='channels_last'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "    model.add(Conv2D(256, kernel_size=7, strides=(3, 3), \n",
    "                      data_format='channels_last',kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(256, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "    model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "    model.add(Dense(4))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building model\n",
    "print(\"----------------------------------------------------------\")\n",
    "adam = Adam(lr=0.0002)\n",
    "model = get_model()\n",
    "with open('./model_summary.txt', 'w') as model_summary:\n",
    "    model.summary(print_fn = lambda x: model_summary.write(x + '\\n'))\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "print(\"Model compiled\")\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global parameters for getting data and storing valuable information\n",
    "## This loads batches of 100 resized images from DATADIR ./data\n",
    "DATADIR_PREFIX = './data/X_resized_train_batch_'\n",
    "DATADIR_SUFFIX = '.npz'\n",
    "## Saves model after each 1000 sample's batch\n",
    "MODEL_PREFIX = './saved_weights/batch_'\n",
    "MODEL_SUFFIX = '.h5'\n",
    "## Stores loss for each of the 1000 sample's batch\n",
    "LOSS_PREFIX = './losses/loss_batch_'\n",
    "LOSS_SUFFIX = '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We want to store losses after each mini-batch is over\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop \n",
    "Our main training loop<br>\n",
    "Here's how it works :\n",
    "1. Loads a batch of 1000 resized images from data directory into memory\n",
    "1. Fits the model on those 1000 samples using a minibatch size of 16(max of what we can fit in our memory)\n",
    "1. Because of our LossHistory callback, we can see our loss after each mini-batch\n",
    "1. Saves the model into proper directory with proper batch name and epoch name\n",
    "1. Predicts the values of 4 samples from the training batch itself and writes the predicted values and summary in a file called `training_summary.txt`. We constantly monitor this file, if we find that any of the predictions are going abnormal, we can stop the training for investigating more.\n",
    "1. Finally, removes the currently loaded batch from memory thus making room for new batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let the training begin !\n",
    "print(\"----------------------------------------------------------\")\n",
    "for i in range(14):\n",
    "    X_train = np.load(DATADIR_PREFIX + str(i + 1) + DATADIR_SUFFIX)\n",
    "    X_train = X_train['arr_0']\n",
    "    print(\"Dataset loaded for batch {}, shape {}\".format(i + 1, X_train.shape))\n",
    "    y_low = i * 1000\n",
    "    y_high = (i + 1) * 1000\n",
    "    losshistory = LossHistory()\n",
    "    hist = model.fit(X_train, Y_train[y_low:y_high], epochs=1, batch_size=16, callbacks=[losshistory])\n",
    "    print(hist)\n",
    "    print(\"Batch {} completed\".format(i + 1))\n",
    "    print(\"Saving weights...\")\n",
    "    model.save(MODEL_PREFIX + str(i + 1) + MODEL_SUFFIX)\n",
    "    print(\"Saving loss diagram...\")\n",
    "    plt.plot(losshistory.losses)\n",
    "    plt.savefig(LOSS_PREFIX + str(i + 1) + LOSS_SUFFIX)\n",
    "    Y_pred = model.predict(X_train[0:4])\n",
    "    with open('training_summary.txt', \"a\") as ts:\n",
    "        ts.write(\"Batch: {}\\n\".format(i + 1))\n",
    "        ts.write(\"Starting Loss: {}\\n\".format(losshistory.losses[0]))\n",
    "        ts.write(\"Ending Loss: {}\\n\".format(losshistory.losses[-1]))\n",
    "        ts.write(\"Predictions : \\n {} \\n\".format(Y_pred))\n",
    "    del X_train\n",
    "    print(\"Batch completed !\")\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Visualizations \n",
    "We trained our model for 18 epochs on our laptops within 5 days. We could not do it for more epochs due to our mid semester exams. The training gave us the score of 79.93% on leaderboard. Following sections describe the visualization methodology used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from keras.models import load_model\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time \n",
    "import matplotlib.patches as patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './saved_weights/epoch_18/batch_14.h5'\n",
    "DATA_RESIZED_PATH = './data/X_resized_train_batch_{}.npz'\n",
    "DATA_ORIG_PATH = './data/X_train_batch_{}.0.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting IoU from bounding boxes\n",
    "def get_bouding_box_and_iou(y_pred, y_orig):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union of 2 bounding boxes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : array : [x1, y1, width, height]\n",
    "    y_orig : array : [x1, y1, width, height]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float iou in [0., 1.]\n",
    "    \"\"\"\n",
    "    bb1 = {\n",
    "        'x1': y_pred[0],\n",
    "        'x2': y_pred[0] + y_pred[2],\n",
    "        'y1': y_pred[1],\n",
    "        'y2': y_pred[1] + y_pred[3]\n",
    "    }\n",
    "    bb2 = {\n",
    "        'x1': y_orig[0],\n",
    "        'x2': y_orig[0] + y_orig[2],\n",
    "        'y1': y_orig[1],\n",
    "        'y2': y_orig[1] + y_orig[3]\n",
    "    }\n",
    "    print(bb1)\n",
    "    print(bb2)\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.read_csv('./training')\n",
    "Y_train = Y_train.drop('image_name', axis=1)\n",
    "Y_train['width'] = Y_train['x2'] - Y_train['x1']\n",
    "Y_train['height'] = Y_train['y2'] - Y_train['y1']\n",
    "Y_train = Y_train.drop('x2', axis=1)\n",
    "Y_train = Y_train.drop('y2', axis=1)\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "fact = 10\n",
    "cols = 4\n",
    "fig, ax = plt.subplots(4, cols, figsize=(4 * fact, cols * fact))\n",
    "for i in range(cols):\n",
    "    X_resized_train = np.load(DATA_RESIZED_PATH.format(i + 1))\n",
    "    X_resized_train = X_resized_train['arr_0']\n",
    "    X_train = np.load(DATA_ORIG_PATH.format(i + 1))\n",
    "    y_low = i * 1000\n",
    "    Y_pred = model.predict(X_resized_train[0:4])\n",
    "    for j in range(4):\n",
    "        iou = get_bouding_box_and_iou(Y_pred[j], Y_train.iloc[y_low + j])\n",
    "        # Showing the original image\n",
    "        ax[j][i].imshow(X_train[j])\n",
    "        x1, y1, width, height = Y_train.iloc[y_low + j]\n",
    "        x1d, y1d, widthd, heightd = Y_pred[j]\n",
    "        rect = patches.Rectangle((x1, y1), width, height, linewidth=1, edgecolor='g', facecolor='none')\n",
    "        rectd = patches.Rectangle((x1d, y1d), widthd, heightd, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax[j][i].add_patch(rect)\n",
    "        ax[j][i].add_patch(rectd)\n",
    "        ax[j][i].set_title('IoU = {}'.format(iou))\n",
    "        ax[j][i].axis('off')\n",
    "    del X_resized_train\n",
    "    del X_train \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss decreased to ~10000 after first epoch. The figure below shows the losses for each mini batch during during 1st epoch.Whereas figure right to it shows the losses for last epoch's mini batches(Pardon for not putting correct labels on the graph images. The X axis denotes the mini-batch number while Y-axis denotes loss value for that mini-batch)<br>\n",
    "\n",
    "| Losses in 1st epoch | Losses in last(18th) epoch |\n",
    "| -- | -- |\n",
    "| ![Losses in 2nd epoch graph](https://raw.githubusercontent.com/ashutoshbsathe/DLNotebooks/b457b73e697daecdf2ef2b00b743912935417a7c/images/flipkart_grid/loss_batch_1.png) | ![Losses in last(18th) epoch graph](https://raw.githubusercontent.com/ashutoshbsathe/DLNotebooks/b457b73e697daecdf2ef2b00b743912935417a7c/images/flipkart_grid/loss_batch_14.png) |\n",
    "\n",
    "**Visualizations of predicted bounding box**\n",
    "![Visualizations of IoU combined image](https://raw.githubusercontent.com/ashutoshbsathe/DLNotebooks/master/images/flipkart_grid/flipkart_grid_visualizations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Words \n",
    "Our model seems to be performing good enough for a small shallow model. As we can see from the visualizations,the model seems to be getting wrong bounding box for the images with humans in them. This could be because of multiple reasons such as \n",
    "\n",
    "1. Shallow model\n",
    "1. Lesser training time\n",
    "1. Only a fraction of images contain humans in them \n",
    "\n",
    "With more data, some of these issues may get solved. <br><br>\n",
    "For some of the images (specifically 1st column 4th image and last column 4th image), our model seems to not recognize the white part correctly. This was tough for even us to recognize the white sole of the shoes correctly. On the other hand, white shoes in images (3rd column, 2nd image from top and 4th column, bottommost image) seem to have been recognized at higher accuracy. This may mean that given enough time, the model may perform better in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
